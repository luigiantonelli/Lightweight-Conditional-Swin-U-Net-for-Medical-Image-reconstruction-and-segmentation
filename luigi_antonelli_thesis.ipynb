{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luigiantonelli/Lightweight-Conditional-Swin-U-Net-for-Medical-Image-reconstruction-and-segmentation/blob/main/luigi_antonelli_thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Author: Luigi Antonelli\n",
        "\n",
        "Implementation in PyTorch (and PyTorch Lightning) of the Deep Learning architectures introduced in the Master Thesis \"Lightweight Conditional Swin U-Net for Medical Image reconstruction and segmentation.\n",
        "\n",
        "Note: the notebook is not ready to run since it has been modified to hide details about my personal file system and my Weights & Biases credentials.\n",
        "\n",
        "Autore: Luigi Antonelli\n",
        "\n",
        "Implementazione in PyTorch (e PyTorch Lightning) delle architetture di Deep Learning introdotte nella tesi di Laurea Magistrale \"Lightweight Conditional Swin U-Net for Medical Image reconstruction and segmentation\".\n",
        "\n",
        "Nota: il notebook non è pronto per l'esecuzione poiché è stato modificato per nascondere i dettagli del mio file system personale e le mie credenziali di Weights & Biases."
      ],
      "metadata": {
        "id": "ZCvZxTcC0Z2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and installations"
      ],
      "metadata": {
        "id": "IaBlGxHb_w7S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiyiiwHOm8mG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e51b57cf-1783-4ff0-a562-a08a29d89aba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m715.6/715.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.4/101.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for runstats (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning==2.0.0 --quiet\n",
        "!pip install torchmetrics --quiet\n",
        "!pip install torchvision --quiet\n",
        "!pip install gdown==4.5.4 --no-cache-dir --quiet\n",
        "!pip install einops --quiet\n",
        "!pip install fastmri --quiet\n",
        "!pip install h5py --quiet\n",
        "!pip install nibabel --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import math\n",
        "import pickle\n",
        "import random\n",
        "from typing import *\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "import nibabel as nib\n",
        "import pandas as pd\n",
        "import h5py\n",
        "from scipy.ndimage import zoom\n",
        "import gdown\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "#from torchvision.transforms import RandomHorizontalFlip, RandomVerticalFlip, RandomResizedCrop, RandomRotation\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import multiprocessing as mp\n",
        "\n",
        "import torchmetrics\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer, seed_everything\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "import fastmri\n",
        "from fastmri.data import transforms as fastmriT\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "seed_everything(10, workers=True)\n"
      ],
      "metadata": {
        "id": "9ejQiGO__bqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8ea7b16-a75a-42db-9953-e3a4ee8889d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Global seed set to 10\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fastMRI dataset"
      ],
      "metadata": {
        "id": "tcAAFKTE_3Cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mask:\n",
        "    def __init__(self, acceleration: int):\n",
        "\n",
        "        self.__acceleration = acceleration\n",
        "\n",
        "    @property\n",
        "    def acceleration(self):\n",
        "        return self.__acceleration\n",
        "\n",
        "    def __call__(self, shape: Tuple) -> np.ndarray:\n",
        "        if len(shape) == 3:\n",
        "            n_sample, n_row, n_col = shape\n",
        "        elif len(shape) == 2:\n",
        "            n_sample = 1\n",
        "            n_row, n_col = shape\n",
        "        else:\n",
        "            raise ValueError(\"Shape should have 2 or 3 dimensions\")\n",
        "\n",
        "        return self.generate_mask(n_sample, n_row, n_col)\n",
        "\n",
        "    def generate_mask(self, n_sample: int,n_row: int, n_col: int) -> np.ndarray:\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "AiKRg8AxW0ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RectangularEquispacedMask(Mask):\n",
        "    def __init__(\n",
        "        self,\n",
        "        acceleration: int,\n",
        "        central_ratio: int):\n",
        "\n",
        "        super().__init__(acceleration)\n",
        "\n",
        "        self.__central_ratio = central_ratio\n",
        "\n",
        "    @property\n",
        "    def central_ratio(self):\n",
        "        return self.__central_ratio\n",
        "\n",
        "    def generate_mask(\n",
        "        self,\n",
        "        n_sample: int,\n",
        "        n_row: int,\n",
        "        n_col: int,\n",
        "        ) -> np.ndarray:\n",
        "\n",
        "        base_mask = np.zeros((n_row, n_col))\n",
        "\n",
        "        n_lines = int(np.floor(n_col/self.acceleration))\n",
        "        n_low = int(np.floor(n_lines*self.central_ratio))\n",
        "        if n_low % 2 != 0:\n",
        "            n_low += 1\n",
        "\n",
        "        start_low = int(n_col/2 - n_low/2)\n",
        "        end_low = int(n_col/2 + n_low/2)\n",
        "        base_mask[:, start_low:end_low] = 1\n",
        "\n",
        "        n_high = n_lines - n_low\n",
        "        if n_high > 0:\n",
        "            high_tab = int(np.around(n_col-n_lines)/(n_high+2))\n",
        "            for h in range(int(n_high/2)):\n",
        "                temp = int(high_tab*(h+1)+h)\n",
        "                base_mask[:, start_low-temp-1] = 1\n",
        "                base_mask[:, end_low+temp] = 1\n",
        "\n",
        "        if np.ones((n_row, n_col)).sum()/base_mask.sum() < self.acceleration:\n",
        "            warnings.warn(\"WARNING: Mask with wrong (less) acceleration\")\n",
        "\n",
        "        if n_sample == 1:\n",
        "            return base_mask\n",
        "\n",
        "        return np.repeat(base_mask[np.newaxis, :, :], n_sample, axis=0)\n"
      ],
      "metadata": {
        "id": "fglSMMGrY9ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "def normalize(\n",
        "    input_array: np.ndarray\n",
        "    ):\n",
        "\n",
        "    if len(input_array.shape) < 2:\n",
        "        raise ValueError(\"Dimension must be at least 2\")\n",
        "\n",
        "    result = input_array.copy()\n",
        "    len_shape = len(result.shape)\n",
        "    result -= result.min(axis=(len_shape-2, len_shape-1), keepdims=True)\n",
        "    _max = result.max(axis=(len_shape-2, len_shape-1), keepdims=True)\n",
        "    _max[_max==0] = np.finfo(np.float64).tiny\n",
        "    result /= _max\n",
        "\n",
        "    return result\n",
        "\n",
        "def apply_mask(\n",
        "    k_space: np.ndarray,\n",
        "    mask: np.ndarray\n",
        "    ):\n",
        "\n",
        "    if k_space.shape != mask.shape:\n",
        "        raise ValueError(\"k_space and mask must have the same shape\")\n",
        "\n",
        "    return np.multiply(k_space, mask)\n",
        "\n",
        "def check_dark(\n",
        "    img: np.ndarray,\n",
        "    dark_threshold: float = 0.25\n",
        "    ):\n",
        "\n",
        "    rows = [0, img.shape[0]]\n",
        "    cols = [0, img.shape[1]]\n",
        "    rows_bool = np.max(img, axis=1) > dark_threshold\n",
        "    cols_bool = np.max(img, axis=0) > dark_threshold\n",
        "\n",
        "    for i, b in enumerate(rows_bool):\n",
        "        if b:\n",
        "            rows[0] = i\n",
        "            break\n",
        "    for i, b in enumerate(np.flip(rows_bool)):\n",
        "        if b:\n",
        "            # rows[1] = img.shape[0] - i\n",
        "            rows[1] = i\n",
        "            break\n",
        "    for i, b in enumerate(cols_bool):\n",
        "        if b:\n",
        "            cols[0] = i\n",
        "            break\n",
        "    for i, b in enumerate(np.flip(cols_bool)):\n",
        "        if b:\n",
        "            # cols[1] = img.shape[1] - i\n",
        "            cols[1] = i\n",
        "            break\n",
        "\n",
        "    return (rows, cols)\n",
        "\n",
        "def check_dark_square(\n",
        "    img: np.ndarray,\n",
        "    dark_threshold: float = 0.25\n",
        "    ):\n",
        "\n",
        "    rows, cols = check_dark(img, dark_threshold)\n",
        "\n",
        "    return (int(min(rows)), int(min(cols)))"
      ],
      "metadata": {
        "id": "-CyPwRiGzpQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class fastMRIDataset(Dataset):\n",
        "    def __init__(self, preprocessed_data_path: str,\n",
        "        original_data_path: str = None, mask: Mask = None, dark_threshold = 0.25, keyword: str = 'train',\n",
        "        target_size : int = 256,\n",
        "        max_proc: int = 1,\n",
        "        train: bool = False,\n",
        "        test: bool = False,\n",
        "        max_vlms: int = 20,\n",
        "        pd: bool = False,\n",
        "        pdfs: bool = False):\n",
        "\n",
        "        super().__init__()\n",
        "        self.original_data_path = original_data_path\n",
        "        self.preprocessed_data_path = preprocessed_data_path\n",
        "        self.images = []\n",
        "        self.reconstructed_images = []\n",
        "        self.__max_vlms = max_vlms // max_proc\n",
        "        self.__pd = pd\n",
        "        self.__pdfs = pdfs\n",
        "        self.__target_size = target_size\n",
        "        self.__dark_threshold = dark_threshold\n",
        "        self.__test = test\n",
        "        self.__train = train\n",
        "\n",
        "        if self.original_data_path is not None:\n",
        "\n",
        "            preprocess = True\n",
        "\n",
        "            h5_files = [os.path.join(self.original_data_path, f) for f in os.listdir(self.original_data_path) if '.h5' in f]\n",
        "            if len(h5_files) == 0:\n",
        "                raise Exception(\"No valid h5 files found\")\n",
        "\n",
        "            if os.path.isdir(self.preprocessed_data_path):\n",
        "                temp = [os.path.join(self.preprocessed_data_path, f) for f in os.listdir(self.preprocessed_data_path) if '.npz' in f]\n",
        "\n",
        "                if len(temp) > 0:\n",
        "                    preprocess = False\n",
        "                    warnings.warn(\"WARNING: Preprocessed file already present\")\n",
        "\n",
        "            else:\n",
        "                os.mkdir(self.preprocessed_data_path)\n",
        "\n",
        "            if preprocess:\n",
        "\n",
        "                if not mask:\n",
        "                    mask = RectangularEquispacedMask(acceleration=4, central_ratio=0.8)\n",
        "\n",
        "                if max_proc == 1:\n",
        "                    self._generate_samples(h5_files, mask)\n",
        "                else:\n",
        "                    linspace = np.linspace(start=0, stop=len(h5_files), num=max_proc+1, dtype=int)\n",
        "                    # tqdm from https://stackoverflow.com/questions/66208601/tqdm-and-multiprocessing-python\n",
        "                    lock = mp.Manager().Lock()\n",
        "                    with mp.Pool(processes=max_proc) as pool:\n",
        "                        for i in range(max_proc):\n",
        "                            pool.apply_async(self._generate_samples, args=(h5_files[linspace[i]:linspace[i+1]], mask, i+1, lock))\n",
        "\n",
        "                        pool.close()\n",
        "                        pool.join()\n",
        "\n",
        "        self.preprocessed_data_list = [os.path.join(self.preprocessed_data_path, f) for f in os.listdir(self.preprocessed_data_path) if '.npz' in f]\n",
        "\n",
        "        if len(self.preprocessed_data_list) == 0:\n",
        "            raise Exception(\"No valid npz (preprocesses) files found\")\n",
        "\n",
        "    @property\n",
        "    def keyword(self):\n",
        "        return self.__keyword\n",
        "\n",
        "    @property\n",
        "    def target_size(self):\n",
        "        return self.__target_size\n",
        "\n",
        "    @property\n",
        "    def dark_threshold(self):\n",
        "        return self.__dark_threshold\n",
        "\n",
        "    @property\n",
        "    def train(self):\n",
        "        return self.__train\n",
        "\n",
        "    @property\n",
        "    def test(self):\n",
        "        return self.__test\n",
        "\n",
        "    @property\n",
        "    def preprocessed_data_list(self):\n",
        "        return self.__preprocessed_data_list\n",
        "\n",
        "    @preprocessed_data_list.setter\n",
        "    def preprocessed_data_list(self, value):\n",
        "        self.__preprocessed_data_list = value\n",
        "\n",
        "\n",
        "\n",
        "    def _idxs_dark_slices(self, vlm: np.ndarray):\n",
        "        rows_bools = np.sum((np.max(vlm, axis=2) > self.__dark_threshold), axis=1)\n",
        "        cols_bools = np.sum((np.max(vlm, axis=1) > self.__dark_threshold), axis=1)\n",
        "        slices_list = [True if r/vlm.shape[1] > 0.10 or c/vlm.shape[2] > 0.10 else False for r, c in zip(rows_bools, cols_bools)]\n",
        "\n",
        "        start_idx = 0\n",
        "        end_idx = vlm.shape[0] - 1\n",
        "\n",
        "        for i in range(vlm.shape[0]):\n",
        "            if slices_list[i]:\n",
        "                start_idx = i\n",
        "                break\n",
        "        for i in range(vlm.shape[0]-1, -1, -1):\n",
        "            if slices_list[i]:\n",
        "                end_idx = i\n",
        "                break\n",
        "\n",
        "        return [start_idx, end_idx]\n",
        "\n",
        "    def _crop_pad_2_square(\n",
        "        self,\n",
        "        img: np.ndarray,\n",
        "        img_1: np.ndarray\n",
        "    ):\n",
        "\n",
        "        result = img.copy()\n",
        "        result_1 = img_1.copy()\n",
        "\n",
        "        h, w = result.shape\n",
        "        if h == w:\n",
        "            return (result, result_1)\n",
        "\n",
        "        max_h_cut = result.shape[-2]\n",
        "        max_w_cut = result.shape[-1]\n",
        "        max_h_cut, max_w_cut = check_dark_square(result)\n",
        "\n",
        "        if h > w:\n",
        "            h_cut = int((h-w)/2)\n",
        "            if h_cut <= max_h_cut:\n",
        "                return (result[h_cut:-h_cut,:], result_1[h_cut:-h_cut,:])\n",
        "\n",
        "            w_pad = h_cut\n",
        "            if max_h_cut != 0 and max_h_cut < int((h - self.__target_size)/2):\n",
        "                result = result[max_h_cut:-max_h_cut,:]\n",
        "                result_1 = result_1[max_h_cut:-max_h_cut,:]\n",
        "                h, w = result.shape\n",
        "                h_cut = int((h-w)/2)\n",
        "                w_pad = h_cut\n",
        "            return (np.pad(result, ((0, 0), (w_pad, w_pad)), mode='constant', constant_values=0),\n",
        "                    np.pad(result_1, ((0, 0), (w_pad, w_pad)), mode='constant', constant_values=0))\n",
        "        elif w > h:\n",
        "            w_cut = int((w-h)/2)\n",
        "            if w_cut <= max_w_cut:\n",
        "                return (result[:,w_cut:-w_cut], result_1[:,w_cut:-w_cut])\n",
        "\n",
        "            h_pad = w_cut\n",
        "            if max_w_cut != 0 and max_w_cut < int((w - self.__target_size)/2):\n",
        "                result = result[:,max_w_cut:-max_w_cut]\n",
        "                result_1 = result_1[:,max_w_cut:-max_w_cut]\n",
        "                h, w = result.shape\n",
        "                w_cut = int((w-h)/2)\n",
        "                h_pad = w_cut\n",
        "            return (np.pad(result, ((h_pad, h_pad), (0, 0)), mode='constant', constant_values=0),\n",
        "                    np.pad(result_1, ((h_pad, h_pad), (0, 0)), mode='constant', constant_values=0))\n",
        "\n",
        "    def _gen_background_mask(\n",
        "        self,\n",
        "        sample,\n",
        "        dark_threshold = 0.25\n",
        "    ):\n",
        "\n",
        "        img = torch.tensor(sample)\n",
        "\n",
        "        bg_mask = torch.zeros(img.size())\n",
        "\n",
        "        img_bools = img > dark_threshold\n",
        "        h, w = img_bools.size()\n",
        "\n",
        "        rows_bools = torch.ones(img_bools.size()) * -1\n",
        "        cols_bools = torch.ones(img_bools.size()) * -1\n",
        "        for r in range(h):\n",
        "            _indxs = img_bools[r, :].argwhere()\n",
        "            if len(_indxs) > 0:\n",
        "                rows_bools[r, _indxs[0]:_indxs[-1]] = 0\n",
        "        for c in range(w):\n",
        "            _indxs = img_bools[:, c].argwhere()\n",
        "            if len(_indxs) > 0:\n",
        "                cols_bools[_indxs[0]:_indxs[-1], c] = 0\n",
        "\n",
        "        bg_mask[((rows_bools * cols_bools) - 1).bool()] = 1\n",
        "\n",
        "        return bg_mask.numpy()\n",
        "\n",
        "    def _generate_samples(\n",
        "        self,\n",
        "        h5_files: list,\n",
        "        mask: Mask,\n",
        "        position: int = 1,\n",
        "        lock = None\n",
        "        ):\n",
        "\n",
        "        if lock == None:\n",
        "            lock = mp.Manager().Lock()\n",
        "\n",
        "        with lock:\n",
        "            bar = tqdm(\n",
        "                desc=f'Worker {position}',\n",
        "                # total=len(h5_files),\n",
        "                total=self.__max_vlms,\n",
        "                position=position,\n",
        "                leave=False\n",
        "            )\n",
        "\n",
        "        count = 0\n",
        "        for f in h5_files:\n",
        "\n",
        "            hf = h5py.File(f)\n",
        "\n",
        "            include_sample = True\n",
        "\n",
        "            if include_sample:\n",
        "                vlm_k = hf['kspace'][()]\n",
        "\n",
        "                mask_vlm = mask.generate_mask(*vlm_k.shape)\n",
        "                masked_vlm_k = apply_mask(vlm_k, mask_vlm)\n",
        "\n",
        "                _samples = fastmriT.normalize_instance(fastmriT.center_crop(fastmri.complex_abs(fastmri.ifft2c(fastmriT.to_tensor(masked_vlm_k))), (320, 320)), eps=1e-9)[0].numpy().astype(np.float64)\n",
        "                _labels = fastmriT.normalize_instance(fastmriT.center_crop(fastmri.complex_abs(fastmri.ifft2c(fastmriT.to_tensor(vlm_k))), (320, 320)), eps=1e-9)[0].numpy().astype(np.float64)\n",
        "\n",
        "                for i, (sample, label) in enumerate(zip(_samples, _labels)):\n",
        "\n",
        "                    label, sample = self._crop_pad_2_square(label, sample)\n",
        "                    sample = normalize(zoom(\n",
        "                        sample,\n",
        "                        zoom=(self.target_size/sample.shape[0], self.target_size/sample.shape[1])\n",
        "                    ))\n",
        "                    label = normalize(zoom(\n",
        "                        label,\n",
        "                        zoom=(self.target_size/label.shape[0], self.target_size/label.shape[1])\n",
        "                    ))\n",
        "\n",
        "                    sample_bg_mask = self._gen_background_mask(self._gen_background_mask(sample, min(max(sample.mean(), self.dark_threshold // 2), self.dark_threshold)))\n",
        "\n",
        "                    if sample_bg_mask.sum() / (self.target_size ** 2) > 0.15:\n",
        "\n",
        "                        out_file_name = f.split('/')[-1].split('.')[0] + '_' + str(i)\n",
        "                        out_file = os.path.join(self.preprocessed_data_path, out_file_name)\n",
        "\n",
        "                        if self.test:\n",
        "                            np.savez_compressed(out_file, sample=sample, label=None, sample_bg_mask=sample_bg_mask)\n",
        "                        else:\n",
        "                            np.savez_compressed(out_file, sample=sample, label=label, sample_bg_mask=sample_bg_mask)\n",
        "\n",
        "                with lock:\n",
        "                    bar.update(1)\n",
        "\n",
        "                count += 1\n",
        "\n",
        "            if count == self.__max_vlms:\n",
        "                break\n",
        "\n",
        "        with lock:\n",
        "            bar.close()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.__preprocessed_data_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert idx < self.__len__()\n",
        "\n",
        "        with np.load(self.__preprocessed_data_list[idx]) as data:\n",
        "            sample = data['sample']\n",
        "            label = data['label']\n",
        "            sample_bg_mask = data['sample_bg_mask']\n",
        "\n",
        "        return (sample, label, sample_bg_mask)"
      ],
      "metadata": {
        "id": "TKj-8xaB_8ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def FastMRI_collate_fn(\n",
        "    data_list: List[Tuple[np.ndarray, np.ndarray, np.ndarray]]\n",
        "    ):\n",
        "\n",
        "    samples = list()\n",
        "    labels = list()\n",
        "    sample_bg_masks = list()\n",
        "\n",
        "    for sample, label, sample_bg_mask in data_list:\n",
        "\n",
        "        samples.append(torch.tensor(sample, dtype=torch.float)[None, :, :])\n",
        "        if label is not None:\n",
        "            labels.append(torch.tensor(label, dtype=torch.float)[None, :, :])\n",
        "        else:\n",
        "            labels.append(None)\n",
        "        sample_bg_masks.append(torch.tensor(sample_bg_mask, dtype=torch.float)[None, :, :])\n",
        "\n",
        "    if None in labels:\n",
        "        return (torch.stack(samples), None, torch.stack(sample_bg_masks))\n",
        "\n",
        "    return (torch.stack(samples), torch.stack(labels), torch.stack(sample_bg_masks))"
      ],
      "metadata": {
        "id": "frAhon0Y245a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class fastMRIDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, preprocessed_data_path: str, preprocessed_data_path_test: str,\n",
        "        original_data_path: str = None, original_data_path_test : str = None, mask: Mask = None, dark_threshold = 0.25, keyword: str = 'train',\n",
        "        target_size : int = 256,\n",
        "        max_proc: int = 1,\n",
        "        max_vlms: int = 20,\n",
        "        pd: bool = False,\n",
        "        pdfs: bool = False,\n",
        "        batch_size: int = 32,\n",
        "        use_validation_set: bool = True\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.original_data_path = original_data_path\n",
        "        self.preprocessed_data_path = preprocessed_data_path\n",
        "        self.original_data_path_test = original_data_path_test\n",
        "        self.preprocessed_data_path_test = preprocessed_data_path_test\n",
        "        self.mask = mask\n",
        "        self.dark_threshold = dark_threshold\n",
        "        self.target_size = target_size\n",
        "        self.max_proc = max_proc\n",
        "        self.max_vlms = max_vlms\n",
        "        self.pd = pd\n",
        "        self.pdfs = pd\n",
        "        self.batch_size = batch_size\n",
        "        self.use_validation_set = use_validation_set\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage == \"fit\":\n",
        "            self.fastmri_train = fastMRIDataset(self.preprocessed_data_path,\n",
        "                self.original_data_path, self.mask, dark_threshold = self.dark_threshold, keyword = 'train',\n",
        "                target_size = self.target_size,\n",
        "                max_proc = self.max_proc,\n",
        "                train = True,\n",
        "                max_vlms = self.max_vlms,\n",
        "                pd = self.pd,\n",
        "                pdfs = self.pdfs)\n",
        "\n",
        "            if self.use_validation_set:\n",
        "                self.fastmri_val = fastMRIDataset(self.preprocessed_data_path_test,\n",
        "                    self.original_data_path_test, self.mask, dark_threshold = self.dark_threshold, keyword = 'test',\n",
        "                    target_size = self.target_size,\n",
        "                    max_proc = self.max_proc,\n",
        "                    train = True,\n",
        "                    max_vlms = self.max_vlms,\n",
        "                    pd = self.pd,\n",
        "                    pdfs = self.pdfs)\n",
        "\n",
        "        if stage == \"test\":\n",
        "            self.fastmri_test = fastMRIDataset(self.preprocessed_data_path_test,\n",
        "                self.original_data_path_test, self.mask, dark_threshold = self.dark_threshold, keyword = 'test',\n",
        "                target_size = self.target_size,\n",
        "                max_proc = self.max_proc,\n",
        "                test = True,\n",
        "                max_vlms = self.max_vlms,\n",
        "                pd = self.pd,\n",
        "                pdfs = self.pdfs)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.fastmri_train, batch_size=self.batch_size, collate_fn=FastMRI_collate_fn,\n",
        "                          shuffle=True, num_workers=2, pin_memory=True, drop_last = True if not self.use_validation_set else False)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        if self.use_validation_set:\n",
        "            return DataLoader(self.fastmri_val, batch_size=self.batch_size, collate_fn=FastMRI_collate_fn,\n",
        "                            num_workers=2, pin_memory=True)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.fastmri_test, batch_size=self.batch_size, collate_fn=FastMRI_collate_fn,)\n",
        "\n",
        "    def teardown(self, stage: str):\n",
        "        # Used to clean-up when the run is finished\n",
        "        pass"
      ],
      "metadata": {
        "id": "o877yFpkAUZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Liver Tumor Segmentation dataset"
      ],
      "metadata": {
        "id": "eFAINFmXY74r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import skimage.io as io\n",
        "import skimage.transform as trans\n",
        "import sys"
      ],
      "metadata": {
        "id": "7g_b68D2wEvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir ./LiverTumorSegmentation/train_images_preprocessed"
      ],
      "metadata": {
        "id": "dcvxqQVqVr6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir ./LiverTumorSegmentation/train_masks_preprocessed"
      ],
      "metadata": {
        "id": "gR38bWgzVw8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def preprocess_directory(df):\n",
        "#     filtered_df = []\n",
        "#     images_dir = \"./LiverTumorSegmentation/train_images_preprocessed\"\n",
        "#     masks_dir = \"./LiverTumorSegmentation/train_masks_preprocessed\"\n",
        "#     threshold_values = [0.0, 128.0, 255.0]\n",
        "#     last_i = 0\n",
        "\n",
        "#     for i in range(len(df)):\n",
        "#         img_name = df.iloc[i, 0]\n",
        "#         mask_name = df.iloc[i, 1]\n",
        "#         image = io.imread(img_name)\n",
        "#         mask = io.imread(mask_name)\n",
        "#         mask_orig = mask.copy()\n",
        "#         new_mask = torch.zeros(*mask.shape)\n",
        "#         for idx, t in enumerate(threshold_values):\n",
        "#             new_mask[(mask >= t - 15) & (mask <= t + 15)] = idx\n",
        "#         if not torch.all(new_mask == 0):\n",
        "#             filtered_df.append(df.iloc[i])\n",
        "\n",
        "#             image_path = os.path.join(images_dir, os.path.basename(img_name))\n",
        "#             io.imsave(image_path, image)\n",
        "\n",
        "#             mask_path = os.path.join(masks_dir, os.path.basename(mask_name))\n",
        "#             io.imsave(mask_path, mask_orig)\n",
        "#         last_i = i\n",
        "#         print(f\"last index: {last_i}\")\n",
        "\n",
        "#     return filtered_df\n",
        "\n",
        "\n",
        "# df_filtered = preprocess_directory(df)"
      ],
      "metadata": {
        "id": "YU3s-NL6N2Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file_list = []\n",
        "# mask_list = []\n",
        "# for dirname, _, filenames in os.walk('./LiverTumorSegmentation/train_images_preprocessed'):\n",
        "#     for filename in filenames:\n",
        "#         file_list.append(filename)\n",
        "\n",
        "# for dirname, _, filenames in os.walk('./LiverTumorSegmentation/train_masks_preprocessed'):\n",
        "#     for filename in filenames:\n",
        "#         mask_list.append(filename)\n",
        "\n",
        "# files = pd.DataFrame({\"train\": file_list, \"mask\": mask_list})\n",
        "\n",
        "# filename = []\n",
        "# for i in range(len(files)):\n",
        "#     root = \"./LiverTumorSegmentation/train_images_preprocessed\"\n",
        "#     path = os.path.join(root, files[\"train\"][i])\n",
        "#     filename.append(path)\n",
        "\n",
        "# mask = []\n",
        "# for i in range(len(files)):\n",
        "#     root = \"./LiverTumorSegmentation/train_masks_preprocessed\"\n",
        "#     path = os.path.join(root, files[\"train\"][i])\n",
        "#     mask.append(path)\n",
        "\n",
        "# df = pd.DataFrame(data={\"filename\": filename, 'mask' : mask})\n",
        "# df['mask'] = df['mask'].str.split(\".\").str[0] + \"_mask.jpg\""
      ],
      "metadata": {
        "id": "SlTdrDmA7JIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = (256, 256)\n",
        "\n",
        "class LiverTumorSegmentationDataset(Dataset):\n",
        "    def __init__(self, dataframe, img_size):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transforms.Resize(img_size, interpolation=transforms.InterpolationMode.NEAREST_EXACT)\n",
        "        self.threshold_values = [0.0, 128.0, 255.0]  # 0 is background, 128 is liver and 255 is tumor. Mapped to 0, 1, 2 respectively for class indices\n",
        "\n",
        "    def threshold_function(self, mask):\n",
        "        new_mask = torch.zeros(*mask.shape)\n",
        "        for idx, t in enumerate(self.threshold_values):\n",
        "            new_mask[(mask >= t - 15) & (mask <= t + 15)] = idx\n",
        "        return new_mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert idx < self.__len__()\n",
        "\n",
        "        img_name = self.dataframe.iloc[idx, 0]  # Assuming the first column is the image filename\n",
        "        mask_name = self.dataframe.iloc[idx, 1]  # Assuming the second column is the mask filename\n",
        "        image = io.imread(img_name)\n",
        "        mask = io.imread(mask_name)\n",
        "\n",
        "        image = np.transpose(image, (2, 0, 1)).astype(np.float32) / 255.0\n",
        "        image = torch.tensor(image, dtype=torch.float32)\n",
        "\n",
        "        mask = self.threshold_function(mask[...,-1])\n",
        "\n",
        "        image = self.transform(image.unsqueeze(0)).squeeze(0)\n",
        "        mask = self.transform(mask.unsqueeze(0)).squeeze(0)\n",
        "        return image, mask.type(torch.long)"
      ],
      "metadata": {
        "id": "Nm1wzGpYT6v9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LiverTumorSegmentationDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, train_dataframe, validation_dataframe = None, test_dataframe = None,\n",
        "        batch_size: int = 32,\n",
        "        use_validation_set: bool = True\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.train_dataframe = train_dataframe\n",
        "        self.validation_dataframe = validation_dataframe\n",
        "        self.test_dataframe = test_dataframe\n",
        "        self.batch_size = batch_size\n",
        "        self.use_validation_set = use_validation_set\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage == \"fit\":\n",
        "            self.livertumorseg_train = LiverTumorSegmentationDataset(dataframe = self.train_dataframe, img_size = img_size)\n",
        "\n",
        "            if self.use_validation_set and self.validation_dataframe is not None:\n",
        "                self.livertumorseg_val = LiverTumorSegmentationDataset(dataframe = self.validation_dataframe, img_size = img_size)\n",
        "\n",
        "        if stage == \"test\":\n",
        "            if self.test_dataframe is not None:\n",
        "                self.livertumorseg_test = LiverTumorSegmentationDataset(dataframe = self.test_dataframe, img_size = img_size)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.livertumorseg_train, batch_size=self.batch_size,\n",
        "                          shuffle=True, pin_memory=True, drop_last = True if not self.use_validation_set else False)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        if self.use_validation_set:\n",
        "            return DataLoader(self.livertumorseg_val, batch_size=self.batch_size, pin_memory=True)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.livertumorseg_test, batch_size=self.batch_size)\n",
        "\n",
        "    def teardown(self, stage: str):\n",
        "        # Used to clean-up when the run is finished\n",
        "        pass"
      ],
      "metadata": {
        "id": "mvkxw3BUjiEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Swin Transformer modules"
      ],
      "metadata": {
        "id": "icHl9zo4l3eL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size, patch_size, input_channels, embedding_dim):\n",
        "        super(PatchEmbedding, self).__init__()\n",
        "        self.img_size, self.patch_size = img_size, patch_size\n",
        "\n",
        "        self.input_channels = input_channels\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, embedding_dim, kernel_size = self.patch_size, stride = self.patch_size),\n",
        "            Rearrange('b d h w -> b (h w) d'),\n",
        "            nn.LayerNorm(embedding_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        batch_size, channels, height, width = img.shape\n",
        "        assert channels == self.input_channels and height == self.img_size[0] and width == self.img_size[1]\n",
        "        return self.projection(img)"
      ],
      "metadata": {
        "id": "QOtypj6qCPwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def window_partition(x, window_size): #divide la sequenza di patch in finestre\n",
        "    # b, h, w, d = x.shape\n",
        "    # x = x.view(b, h // window_size, window_size, w // window_size, window_size, d)\n",
        "    # windows = x.transpose(2,3).contiguous().view(-1, window_size, window_size, d)\n",
        "    windows = rearrange(x, \"b (h ws_0) (w ws_1) d -> (b h w) ws_0 ws_1 d\", ws_0 = window_size, ws_1 = window_size)\n",
        "    return windows\n",
        "\n",
        "def window_merge(windows, window_size, height, width): #ricompone la sequenza di patch a partire dalle finestre\n",
        "    h, w = height // window_size, width // window_size\n",
        "    x = rearrange(windows, \"(b h w) ws_0 ws_1 d -> b (h ws_0) (w ws_1) d\", h = h, w = w, ws_0 = window_size, ws_1 = window_size)\n",
        "    return x"
      ],
      "metadata": {
        "id": "_oT_eU_kU6n9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value, scale, bias = 0, mask = None, dropout_layer = None):\n",
        "    t = (torch.einsum('b h q d, b h k d -> b h q k', query, key) / scale) + bias\n",
        "    # print(f\"t.shape: {t.shape}\")\n",
        "    if mask is not None:\n",
        "        b, h, q, _ = query.shape\n",
        "        k = key.size(2)\n",
        "        n_w = mask.size(1)\n",
        "        t = t.view(b // n_w, n_w, h, q, k) #necessary because each window has a different mask and we wouldn't be able to broadcast the batch dimension\n",
        "        t = t.masked_fill(mask == False, -1e10)\n",
        "        t = t.view(-1, h, q, k)\n",
        "    t = F.softmax(t, dim = -1)\n",
        "    if dropout_layer is not None:\n",
        "        t = dropout_layer(t)\n",
        "    return torch.matmul(t, value)"
      ],
      "metadata": {
        "id": "4Y4vBIYMI0As"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WindowMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, window_size, dropout=0.2):\n",
        "        super(WindowMultiHeadAttention, self).__init__()\n",
        "        assert embedding_dim % num_heads == 0\n",
        "        self.dim_head = embedding_dim // num_heads # Single head dimension\n",
        "        self.sqrt_q = math.sqrt(self.dim_head)\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "\n",
        "        self.W_qkv = nn.Linear(embedding_dim, 3*embedding_dim, bias=True)\n",
        "        self.W_o = nn.Linear(embedding_dim, embedding_dim, bias=True)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.relative_position_table = nn.Parameter(torch.zeros(((2 * window_size - 1) * (2 * window_size - 1), num_heads)))\n",
        "        #(2 * M - 1) perché è il numero di possibili posizioni relative su ogni asse: |{-M+1, ..., M-1}| = 2M-1\n",
        "        #con M = 3: |{-2, -1, 0, 1, 2}| = 5 = 2*3 -1\n",
        "        #anche se relative_position_index è quadratico si risparmia perché ci sono meno parametri da imparare\n",
        "\n",
        "        coords_h = torch.arange(window_size)\n",
        "        coords_w = torch.arange(window_size)\n",
        "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, W, W\n",
        "        coords_flatten = torch.flatten(coords, 1)  # 2, W * W\n",
        "        relative_coords = coords_flatten.unsqueeze(2) - coords_flatten.unsqueeze(1)  # 2, W * W, W * W\n",
        "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # W * W, W * W, 2\n",
        "        relative_coords[:, :, 0] += window_size - 1  # shift to start from 0\n",
        "        relative_coords[:, :, 1] += window_size - 1\n",
        "        relative_coords[:, :, 0] *= window_size - 1\n",
        "        relative_position_index = relative_coords.sum(-1).view(-1)  # W * W, W * W and then flatten\n",
        "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "\n",
        "        qkv = rearrange(self.W_qkv(x), \"b n (qkv n_h d_h) -> qkv b n_h n d_h\", qkv = 3, n_h = self.num_heads, d_h = self.dim_head)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        relative_position_bias = self.relative_position_table[self.relative_position_index]\n",
        "        relative_position_bias = rearrange(relative_position_bias,\n",
        "                                           \"(ws_0 ws_1 ws_2 ws_3) n_h -> 1 n_h (ws_0 ws_1) (ws_2 ws_3)\",\n",
        "                                           ws_0 = self.window_size, ws_1 = self.window_size,\n",
        "                                           ws_2 = self.window_size, ws_3 = self.window_size)\n",
        "\n",
        "        attention_value = scaled_dot_product_attention(q, k, v, scale = self.sqrt_q,\n",
        "                                                       bias = relative_position_bias,\n",
        "                                                       mask = mask, dropout_layer = self.dropout)\n",
        "\n",
        "        attention_value = rearrange(attention_value, \"b h n d_h -> b n (h d_h)\", d_h = self.dim_head)\n",
        "        return self.W_o(attention_value)"
      ],
      "metadata": {
        "id": "rLp29yBpl9Wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinTransformerBlock(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_size, patch_resolution, num_heads, window_size, shift_size=0, dropout=0.2):\n",
        "        super(SwinTransformerBlock, self).__init__()\n",
        "        self.height, self.width = patch_resolution\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "\n",
        "        self.norm_layer1 = nn.LayerNorm(embedding_dim)\n",
        "        self.attention = WindowMultiHeadAttention(embedding_dim, num_heads, window_size, dropout)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.norm_layer2 = nn.LayerNorm(embedding_dim)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, hidden_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, embedding_dim)\n",
        "        )\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "        if self.shift_size > 0:\n",
        "            img_mask = torch.zeros((self.height, self.width))\n",
        "            h_slices = (slice(0, -self.window_size),\n",
        "                        slice(-self.window_size, -self.shift_size),\n",
        "                        slice(-self.shift_size, None))\n",
        "            w_slices = (slice(0, -self.window_size),\n",
        "                        slice(-self.window_size, -self.shift_size),\n",
        "                        slice(-self.shift_size, None))\n",
        "            id = 0\n",
        "            for h in h_slices:#valido solo se lo shift viene fatto con (-self.shift_size,-self.shift_size)\n",
        "                for w in w_slices:\n",
        "                    img_mask[h, w] = id\n",
        "                    id += 1\n",
        "\n",
        "            mask_windows = rearrange(img_mask,\n",
        "                                     \"(h_w ws_0) (w_w ws_1) -> (h_w w_w) (ws_0 ws_1)\",\n",
        "                                     ws_0 = self.window_size, ws_1 = self.window_size)\n",
        "            attention_mask = (mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)) != 0\n",
        "            # attention_mask is a [num_windows, patches_per_window, patches_per_window] tensor\n",
        "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(0)\n",
        "            # attention_mask is a [1, num_windows, 1, patches_per_window, patches_per_window] tensor\n",
        "            # the ones are for broadcasting the batch size and the number of heads\n",
        "        else:\n",
        "            attention_mask = None\n",
        "\n",
        "        self.register_buffer(\"attention_mask\", attention_mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        n = x.size(1)\n",
        "        if n != self.height*self.width:\n",
        "            print(f\"error: x.shape = {x.shape}, height = {self.height}, width = {self.width}\")\n",
        "        assert n == self.height*self.width\n",
        "\n",
        "        z = x\n",
        "        x = rearrange(self.norm_layer1(x), \"b (h w) d -> b h w d\", h = self.height)\n",
        "\n",
        "        if self.shift_size > 0: #for sw-mha\n",
        "            x = torch.roll(x, shifts = (-self.shift_size, -self.shift_size), dims = (1, 2))\n",
        "\n",
        "        windows = window_partition(x, self.window_size)\n",
        "        windows = rearrange(windows, \"bhw ws_0 ws_1 d -> bhw (ws_0 ws_1) d\", ws_0 = self.window_size)\n",
        "\n",
        "        attention = rearrange(self.attention(windows, self.attention_mask),\n",
        "                              \"bhw (ws_0 ws_1) d -> bhw ws_0 ws_1 d\", ws_0 = self.window_size)\n",
        "\n",
        "        x = window_merge(attention, self.window_size, self.height, self.width)\n",
        "\n",
        "        if self.shift_size > 0: #undo shift\n",
        "            x = torch.roll(x, shifts = (self.shift_size, self.shift_size), dims = (1, 2))\n",
        "\n",
        "        x = rearrange(x, \"b h w d -> b (h w) d\")\n",
        "        x = z + self.dropout1(x)\n",
        "\n",
        "        return x + self.dropout2(self.ff(self.norm_layer2(x)))"
      ],
      "metadata": {
        "id": "OMHhQ2foXlOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchMerge(nn.Module):\n",
        "    def __init__(self, patch_resolution, embedding_dim, use_norm_layer = False):\n",
        "        super(PatchMerge, self).__init__()\n",
        "        self.height, self.width = patch_resolution\n",
        "        assert self.height % 2 == 0 and self.width % 2 == 0\n",
        "\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.LayerNorm(4*embedding_dim) if use_norm_layer else nn.Identity(),\n",
        "            nn.Linear(4*embedding_dim, 2*embedding_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        n = x.size(1)\n",
        "        assert n == self.height*self.width\n",
        "\n",
        "        x = rearrange(x, \"b (h w) d -> b h w d\", h = self.height)\n",
        "        x0 = x[:, 0::2, 0::2, :]  #b h/2 w/2 d\n",
        "        x1 = x[:, 1::2, 0::2, :]  #b h/2 w/2 d\n",
        "        x2 = x[:, 0::2, 1::2, :]  #b h/2 w/2 d\n",
        "        x3 = x[:, 1::2, 1::2, :]  #b h/2 w/2 d\n",
        "        x = torch.cat([x0, x1, x2, x3], -1)  #b h/2 w/2 4*d\n",
        "        x = rearrange(x, \"b half_h half_w d4 -> b (half_h half_w) d4\")\n",
        "\n",
        "        return self.projection(x)\n",
        "\n",
        "\n",
        "class PatchExpand(nn.Module):\n",
        "    def __init__(self, patch_resolution, embedding_dim, use_norm_layer = False, is_final = False):\n",
        "        super(PatchExpand, self).__init__()\n",
        "        self.height, self.width = patch_resolution\n",
        "\n",
        "        scale = 2 if not is_final else 4\n",
        "        expanded_dim = 2*embedding_dim if not is_final else 16*embedding_dim\n",
        "        norm_dim = embedding_dim // scale if not is_final else embedding_dim\n",
        "\n",
        "        self.expand_and_rearrange = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, expanded_dim, bias=False),\n",
        "            Rearrange(\"b (h w) d -> b h w d\", h = self.height),\n",
        "            Rearrange(\"b h w (ps_0 ps_1 c) -> b (h ps_0) (w ps_1) c\", h = self.height, ps_0 = scale, ps_1 = scale, c = expanded_dim // (scale**2)),\n",
        "            Rearrange(\"b new_h new_w c -> b (new_h new_w) c\"),\n",
        "            nn.LayerNorm(norm_dim) if use_norm_layer else nn.Identity(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        n = x.size(1)\n",
        "        assert n == self.height*self.width\n",
        "\n",
        "        return self.expand_and_rearrange(x)"
      ],
      "metadata": {
        "id": "WeXn_HAiKL8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conditional modules"
      ],
      "metadata": {
        "id": "oeRDY0HaUhTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batch_range(t, right_pad_dims = 1):\n",
        "    b, device = t.shape[0], t.device\n",
        "    batch_range = torch.arange(b, device = device)\n",
        "    pad_dims = ((1,) * right_pad_dims)\n",
        "    return batch_range.reshape(-1, *pad_dims)\n",
        "\n",
        "def batched_gather(x, indices):\n",
        "    batch_range = create_batch_range(indices, indices.ndim - 1)\n",
        "    return x[batch_range, indices]\n",
        "\n",
        "def route_back(x, routed_tokens, indices):\n",
        "    batch_range = create_batch_range(routed_tokens)\n",
        "    x[batch_range, indices] = routed_tokens\n",
        "    return x"
      ],
      "metadata": {
        "id": "LwzyPXHcUcHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LightRouter(nn.Module):\n",
        "    def __init__(self, embedding_dim, k):\n",
        "        super(LightRouter, self).__init__()\n",
        "        self.k = k\n",
        "        self.routing_embeddings = nn.Parameter(torch.randn(embedding_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        scores = torch.einsum(\"b n d, d -> b n\", x, self.routing_embeddings)\n",
        "        normalized_scores = F.softmax(scores, dim = -1)\n",
        "\n",
        "        k = min(self.k, x.size(1))\n",
        "        values, indices = torch.topk(normalized_scores, dim = -1, k = k)\n",
        "        return values.unsqueeze(-1), indices, normalized_scores.unsqueeze(-1) #unsqueeze to broadcast the embedding dimension"
      ],
      "metadata": {
        "id": "T_-gsjgQm9IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IterativeSoftTopkRouter(nn.Module):\n",
        "    def __init__(self, embedding_dim, k, eps = 2e-2, eps_init = None, eps_decay = 1., iters = 20):\n",
        "        super(IterativeSoftTopkRouter, self).__init__()\n",
        "        self.k = k\n",
        "        self.logk = math.sqrt(k)\n",
        "        self.routing_embeddings = nn.Parameter(torch.randn(embedding_dim))\n",
        "        self.eps = eps\n",
        "        self.eps_init = eps_init\n",
        "        self.eps_decay = eps_decay\n",
        "        self.iters = iters\n",
        "\n",
        "    def forward(self, x):\n",
        "        scores = torch.einsum(\"b n d, d -> b n\", x, self.routing_embeddings)\n",
        "        a = 0\n",
        "        b = -scores\n",
        "\n",
        "        current_eps = max(self.eps_init if self.eps_init is not None else self.eps, self.eps)\n",
        "\n",
        "        for _ in range(self.iters):\n",
        "            sb = ((scores + b) / current_eps)\n",
        "\n",
        "            a = current_eps * (self.logk - sb.logsumexp(dim = -1, keepdim = True))\n",
        "            b = -F.relu(scores + a)\n",
        "\n",
        "            current_eps = max(current_eps * self.eps_decay, self.eps)\n",
        "\n",
        "        normalized_scores = ((scores + a + b) / current_eps).exp()\n",
        "\n",
        "        k = min(self.k, x.size(1))\n",
        "        values, indices = torch.topk(normalized_scores, dim = -1, k = k)\n",
        "        return values.unsqueeze(-1), indices, normalized_scores.unsqueeze(-1) #unsqueeze to broadcast the embedding dimension"
      ],
      "metadata": {
        "id": "u920IV3dL-nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalFeedForward(nn.Module):\n",
        "    def __init__(self, input_dim, k, dropout = 0.2, use_iterative_algorithm = False, eps = 2e-2, eps_init = None, eps_decay = 1., iters = 20):\n",
        "        super(ConditionalFeedForward, self).__init__()\n",
        "\n",
        "        self.k = k\n",
        "\n",
        "        if not use_iterative_algorithm:\n",
        "            self.router = LightRouter(input_dim, k)\n",
        "        else:\n",
        "            self.router= IterativeSoftTopkRouter(input_dim, k, eps, eps_init, eps_decay, iters)\n",
        "\n",
        "        self.light_ff = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim, bias=True),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(input_dim, input_dim, bias=True)\n",
        "        )\n",
        "        self.heavy_ff = nn.Sequential(\n",
        "            nn.Linear(input_dim, 4*input_dim, bias=True),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(4*input_dim, input_dim, bias=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        light_output = self.light_ff(x)\n",
        "\n",
        "        values, indices, _ = self.router(x)\n",
        "        x_topk = batched_gather(x, indices)\n",
        "\n",
        "        heavy_output_topk = values * self.heavy_ff(x_topk)\n",
        "        heavy_output = route_back(torch.zeros(x.size(), device = x.device), heavy_output_topk, indices)\n",
        "\n",
        "        return light_output + heavy_output"
      ],
      "metadata": {
        "id": "DCrbxv2XnRgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalWindowAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, window_size, k, dropout=0.2, use_iterative_algorithm = False, eps = 2e-2, eps_init = None, eps_decay = 1., iters = 20):\n",
        "        super(ConditionalWindowAttention, self).__init__()\n",
        "        assert embedding_dim % num_heads == 0\n",
        "        self.k = k\n",
        "\n",
        "        if not use_iterative_algorithm:\n",
        "            self.router_q = LightRouter(embedding_dim, k)\n",
        "            self.router_kv = LightRouter(embedding_dim, k)\n",
        "        else:\n",
        "            self.router_q = IterativeSoftTopkRouter(embedding_dim, k, eps, eps_init, eps_decay, iters)\n",
        "            self.router_kv = IterativeSoftTopkRouter(embedding_dim, k, eps, eps_init, eps_decay, iters)\n",
        "\n",
        "        self.light_attention = WindowMultiHeadAttention(embedding_dim, num_heads // 2, window_size, dropout)\n",
        "\n",
        "        self.heavy_W_q = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.heavy_W_kv = nn.Linear(embedding_dim, 2*embedding_dim)\n",
        "        self.num_heads = num_heads\n",
        "        self.dim_head = embedding_dim // num_heads\n",
        "        self.sqrt_q = math.sqrt(self.dim_head)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.window_size = window_size\n",
        "\n",
        "        self.relative_position_table = nn.Parameter(torch.zeros(((2 * window_size - 1) * (2 * window_size - 1), num_heads)))\n",
        "        #(2 * M - 1) perché è il numero di possibili posizioni relative su ogni asse: |{-M+1, ..., M-1}| = 2M-1\n",
        "        #con M = 3: |{-2, -1, 0, 1, 2}| = 5 = 2*3 -1\n",
        "        #anche se relative_position_index è quadratico si risparmia perché ci sono meno parametri da imparare\n",
        "\n",
        "        coords_h = torch.arange(window_size)\n",
        "        coords_w = torch.arange(window_size)\n",
        "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, W, W\n",
        "        coords_flatten = torch.flatten(coords, 1)  # 2, W * W\n",
        "        relative_coords = coords_flatten.unsqueeze(2) - coords_flatten.unsqueeze(1)  # 2, W * W, W * W\n",
        "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # W * W, W * W, 2\n",
        "        relative_coords[:, :, 0] += window_size - 1  # shift to start from 0\n",
        "        relative_coords[:, :, 1] += window_size - 1\n",
        "        relative_coords[:, :, 0] *= window_size - 1\n",
        "        relative_position_index = relative_coords.sum(-1).view(-1)  # W * W, W * W and then flatten\n",
        "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        light_output = self.light_attention(x, mask)\n",
        "\n",
        "        _, _, normalized_scores_q = self.router_q(x)\n",
        "        values_kv, indices_kv, _ = self.router_kv(x)\n",
        "        x_topk_kv = values_kv * batched_gather(x, indices_kv)\n",
        "\n",
        "        #light(x_i, x) + normalized_scores_q * heavy(x_i, values_kv * x) with values_kv set to zero for non routed tokens\n",
        "        q = rearrange(self.heavy_W_q(x), \"b n (n_h d_h) -> b n_h n d_h\", n_h = self.num_heads, d_h = self.dim_head)\n",
        "        kv = rearrange(self.heavy_W_kv(x_topk_kv), \"b topk (kv n_h d_h) -> kv b n_h topk d_h\", n_h = self.num_heads, d_h = self.dim_head)\n",
        "        k, v = kv[0], kv[1]\n",
        "\n",
        "        relative_position_bias = self.relative_position_table[self.relative_position_index]\n",
        "        relative_position_bias = rearrange(relative_position_bias,\n",
        "                                           \"(ws_0 ws_1 ws_2 ws_3) n_h -> 1 n_h (ws_0 ws_1) (ws_2 ws_3)\",\n",
        "                                           ws_0 = self.window_size, ws_1 = self.window_size,\n",
        "                                           ws_2 = self.window_size, ws_3 = self.window_size)\n",
        "\n",
        "        relative_position_bias = repeat(relative_position_bias, \"1 ... -> b ...\", b = x.size(0))\n",
        "        indices_kv = repeat(indices_kv, \"b topk -> b n_h n topk\", n_h = self.num_heads, n = x.size(1))\n",
        "        relative_position_bias_topk = torch.gather(relative_position_bias, dim = -1, index = indices_kv)\n",
        "\n",
        "\n",
        "        if mask is None:\n",
        "            mask_topk = None\n",
        "        else:\n",
        "            indices_kv = rearrange(indices_kv, \"(b_o n_w) ... -> b_o n_w ...\", b_o = x.size(0) // mask.size(1)) #b_o = b // num_windows\n",
        "            mask = repeat(mask, \"1 n_w 1 ... -> b_o n_w n_h ...\", b_o = x.size(0) // mask.size(1), n_h = self.num_heads)\n",
        "            mask_topk = torch.gather(mask, dim = -1, index = indices_kv)\n",
        "\n",
        "        heavy_output_topk = scaled_dot_product_attention(q, k, v,\n",
        "                                                         scale = self.sqrt_q, bias = relative_position_bias_topk,\n",
        "                                                         mask = mask_topk, dropout_layer = self.dropout)\n",
        "\n",
        "        heavy_output = normalized_scores_q * rearrange(heavy_output_topk, \"b h n d_h -> b n (h d_h)\", d_h = self.dim_head)\n",
        "        return light_output + heavy_output"
      ],
      "metadata": {
        "id": "xMG4xBcGqfxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalSwinTransformerBlock(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_size, patch_resolution, num_heads, window_size, k, shift_size=0, dropout=0.2, use_iterative_algorithm = False, eps = 2e-2, eps_init = None, eps_decay = 1., iters = 20):\n",
        "        super(ConditionalSwinTransformerBlock, self).__init__()\n",
        "        self.height, self.width = patch_resolution\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "        self.k = k\n",
        "\n",
        "        self.norm_layer1 = nn.LayerNorm(embedding_dim)\n",
        "        self.attention = ConditionalWindowAttention(embedding_dim, num_heads, window_size, k, dropout, use_iterative_algorithm, eps , eps_init, eps_decay, iters)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.norm_layer2 = nn.LayerNorm(embedding_dim)\n",
        "        self.ff = ConditionalFeedForward(embedding_dim, k, dropout, use_iterative_algorithm, eps , eps_init, eps_decay, iters)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "        if self.shift_size > 0:\n",
        "            img_mask = torch.zeros((self.height, self.width))\n",
        "            h_slices = (slice(0, -self.window_size),\n",
        "                        slice(-self.window_size, -self.shift_size),\n",
        "                        slice(-self.shift_size, None))\n",
        "            w_slices = (slice(0, -self.window_size),\n",
        "                        slice(-self.window_size, -self.shift_size),\n",
        "                        slice(-self.shift_size, None))\n",
        "            id = 0\n",
        "            for h in h_slices:#valido solo se lo shift viene fatto con (-self.shift_size,-self.shift_size)\n",
        "                for w in w_slices:\n",
        "                    img_mask[h, w] = id\n",
        "                    id += 1\n",
        "\n",
        "            mask_windows = rearrange(img_mask,\n",
        "                                     \"(h_w ws_0) (w_w ws_1) -> (h_w w_w) (ws_0 ws_1)\",\n",
        "                                     ws_0 = self.window_size, ws_1 = self.window_size)\n",
        "            attention_mask = (mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)) != 0\n",
        "            # attention_mask is a [num_windows, patches_per_window, patches_per_window] tensor\n",
        "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(0)\n",
        "            # attention_mask is a [1, num_windows, 1, patches_per_window, patches_per_window] tensor\n",
        "            # the ones are for broadcasting the batch size and the number of heads\n",
        "        else:\n",
        "            attention_mask = None\n",
        "\n",
        "        self.register_buffer(\"attention_mask\", attention_mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        n = x.size(1)\n",
        "        assert n == self.height*self.width\n",
        "\n",
        "        z = x\n",
        "        x = rearrange(self.norm_layer1(x), \"b (h w) d -> b h w d\", h = self.height)\n",
        "\n",
        "        if self.shift_size > 0: #for sw-mha\n",
        "            x = torch.roll(x, shifts = (-self.shift_size, -self.shift_size), dims = (1, 2))\n",
        "\n",
        "        windows = window_partition(x, self.window_size)\n",
        "        windows = rearrange(windows, \"bhw ws_0 ws_1 d -> bhw (ws_0 ws_1) d\", ws_0 = self.window_size)\n",
        "\n",
        "        attention = rearrange(self.attention(windows, self.attention_mask),\n",
        "                              \"bhw (ws_0 ws_1) d -> bhw ws_0 ws_1 d\", ws_0 = self.window_size)\n",
        "\n",
        "        x = window_merge(attention, self.window_size, self.height, self.width)\n",
        "\n",
        "        if self.shift_size > 0: #undo shift\n",
        "            x = torch.roll(x, shifts = (self.shift_size, self.shift_size), dims = (1, 2))\n",
        "\n",
        "        x = rearrange(x, \"b h w d -> b (h w) d\")\n",
        "        x = z + self.dropout1(x)\n",
        "\n",
        "        return x + self.dropout2(self.ff(self.norm_layer2(x)))"
      ],
      "metadata": {
        "id": "JkPTgm9xq9J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Swin Transformer U-Nets\n",
        "\n"
      ],
      "metadata": {
        "id": "DDP19dVDSzmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicLayer(nn.Module):\n",
        "    def __init__(self, embedding_dim, patch_resolution, depth, num_heads, window_size,\n",
        "                 hidden_size, dropout=0.2, downsample=False, upsample=False, use_conditional_blocks = False, use_iterative_algorithm = False, eps = 2e-2, eps_init = None, eps_decay = 1., iters = 20):\n",
        "        super(BasicLayer, self).__init__()\n",
        "        assert not (downsample and upsample), \"Only one of downsample and upsample can be True\"\n",
        "\n",
        "        if not use_conditional_blocks:\n",
        "            blocks = [\n",
        "                SwinTransformerBlock(embedding_dim, hidden_size,\n",
        "                                    patch_resolution, num_heads, window_size,\n",
        "                                    shift_size = 0 if i % 2 == 0 else window_size // 2, dropout = dropout)\n",
        "                for i in range(depth)\n",
        "            ]\n",
        "        else:\n",
        "            blocks = [\n",
        "                ConditionalSwinTransformerBlock(embedding_dim, hidden_size,\n",
        "                                    patch_resolution, num_heads, window_size, 7,\n",
        "                                    shift_size = 0 if i % 2 == 0 else window_size // 2, dropout = dropout, use_iterative_algorithm = use_iterative_algorithm,\n",
        "                                    eps = eps, eps_init = eps_init, eps_decay = eps_decay, iters = iters)\n",
        "                for i in range(depth)\n",
        "            ]\n",
        "\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "        if downsample:\n",
        "            self.patch_rearrange = PatchMerge(patch_resolution, embedding_dim, use_norm_layer = True)\n",
        "        elif upsample:\n",
        "            self.patch_rearrange = PatchExpand(patch_resolution, embedding_dim, use_norm_layer = True)\n",
        "        else:\n",
        "            self.patch_rearrange = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.blocks(x)\n",
        "        return self.patch_rearrange(x)\n"
      ],
      "metadata": {
        "id": "OaM9G9PnD5O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NMSE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NMSE, self).__init__()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        diff = y - x\n",
        "        error = torch.div(torch.pow(torch.linalg.norm(diff), 2), torch.pow(torch.linalg.norm(y), 2))\n",
        "        return error"
      ],
      "metadata": {
        "id": "ha4rLknoQzKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinTransformer_Unet(pl.LightningModule):\n",
        "    def __init__(self, img_size, patch_size, input_channels, embedding_dim, hidden_size, window_size, output_channels, dropout = 0.2, norm_layer = nn.LayerNorm,\n",
        "                 depths = [2, 2, 2], num_heads = [4, 4, 4, 4], use_conditional_blocks = False, learning_rate = 1e-3, epoch = 0,\n",
        "                 use_iterative_algorithm = False, eps = 2e-2, eps_init = None, eps_decay = 1., iters = 20, fastMRI = True):\n",
        "        super(SwinTransformer_Unet, self).__init__()\n",
        "        def to_tuple(x):\n",
        "            if not isinstance(x, tuple):\n",
        "                return (x, x)\n",
        "            return x\n",
        "\n",
        "        self.wandb_log = {}\n",
        "        self.epoch = epoch\n",
        "        self.train_loss = []\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.img_size, self.patch_size = to_tuple(img_size), to_tuple(patch_size)\n",
        "\n",
        "        assert self.img_size[0] % self.patch_size[0] == 0 and self.img_size[1] % self.patch_size[1] == 0\n",
        "        self.patch_resolution = (self.img_size[0] // self.patch_size[0], self.img_size[1] // self.patch_size[1])\n",
        "        self.num_patches = self.patch_resolution[0] * self.patch_resolution[1]\n",
        "        self.depths = len(depths)\n",
        "\n",
        "        self.patch_embeddings = PatchEmbedding(self.img_size, self.patch_size, input_channels, embedding_dim)\n",
        "\n",
        "        self.encoder = nn.ModuleList([\n",
        "            BasicLayer(int(embedding_dim * 2**i), (self.patch_resolution[0] // (2**i), self.patch_resolution[1] // (2**i)),\n",
        "                       depths[i], num_heads[i],\n",
        "                       window_size, hidden_size, dropout, downsample = True, use_conditional_blocks=use_conditional_blocks,\n",
        "                       use_iterative_algorithm = use_iterative_algorithm,\n",
        "                       eps = eps, eps_init = eps_init, eps_decay = eps_decay, iters = iters)\n",
        "            for i in range(len(depths))\n",
        "        ])\n",
        "\n",
        "        self.bottleneck = BasicLayer(int(embedding_dim * 2**len(depths)), (self.patch_resolution[0] // (2**len(depths)), self.patch_resolution[1] // (2**len(depths))), 2,\n",
        "                                     num_heads[len(depths)], window_size, hidden_size, dropout, use_conditional_blocks=use_conditional_blocks,\n",
        "                                     use_iterative_algorithm = use_iterative_algorithm,\n",
        "                                     eps = eps, eps_init = eps_init, eps_decay = eps_decay, iters = iters)\n",
        "\n",
        "        self.concat_linear = nn.ModuleList([\n",
        "            nn.Linear(2*int(embedding_dim*2**(len(depths)-i-1)), int(embedding_dim*2**(len(depths)-i-1)))\n",
        "            for i in range(len(depths))\n",
        "        ])\n",
        "\n",
        "        self.decoder = nn.ModuleList([\n",
        "            PatchExpand((self.patch_resolution[0] // (2**len(depths)), self.patch_resolution[1] // (2**len(depths))),\n",
        "                        int(embedding_dim * 2**len(depths)), use_norm_layer = True)\n",
        "            ] + [\n",
        "            BasicLayer(int(embedding_dim * 2**(len(depths) - 1 - i)), (self.patch_resolution[0] // (2 ** (len(depths) - 1 - i)), self.patch_resolution[1] // (2 ** (len(depths) - 1 - i))),\n",
        "                    depths[len(depths) - 1 - i], num_heads[len(depths) - 1 - i],\n",
        "                    window_size, hidden_size, dropout, upsample = (i < len(depths) - 1), use_conditional_blocks=use_conditional_blocks,\n",
        "                    use_iterative_algorithm = use_iterative_algorithm,\n",
        "                    eps = eps, eps_init = eps_init, eps_decay = eps_decay, iters = iters)\n",
        "            for i in range(len(depths))\n",
        "            ] + [\n",
        "            PatchExpand(self.patch_resolution, embedding_dim, use_norm_layer = True, is_final = True)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(int(embedding_dim * 2**len(depths)))\n",
        "        self.norm_up = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "        self.output = nn.Conv2d(embedding_dim, output_channels, kernel_size = 1, bias = False)\n",
        "\n",
        "        self.train_mae = torchmetrics.MeanAbsoluteError()\n",
        "\n",
        "        self.validation_mae = torchmetrics.MeanAbsoluteError()\n",
        "        self.validation_psnr = torchmetrics.image.PeakSignalNoiseRatio(data_range=1)\n",
        "        self.validation_ssim = torchmetrics.image.StructuralSimilarityIndexMeasure(kernel_size=7, data_range=1)\n",
        "        self.validation_nmse = NMSE()\n",
        "        self.validation_nmse_errors = []\n",
        "\n",
        "        self.test_mae = torchmetrics.MeanAbsoluteError()\n",
        "\n",
        "        if fastMRI:\n",
        "            self.loss_function = F.mse_loss\n",
        "            self.train_metrics = torchmetrics.MeanAbsoluteError()\n",
        "            self.validation_metrics = nn.ModuleList([torchmetrics.MeanAbsoluteError(),\n",
        "                                       torchmetrics.image.PeakSignalNoiseRatio(data_range=1),\n",
        "                                       torchmetrics.image.StructuralSimilarityIndexMeasure(kernel_size=7, data_range=1),\n",
        "                                       NMSE()])\n",
        "            self.validation_nmse_errors = []\n",
        "            self.test_metrics = torchmetrics.MeanAbsoluteError()\n",
        "        else:\n",
        "            self.loss_function = nn.CrossEntropyLoss()\n",
        "            self.train_metrics = torchmetrics.Dice(num_classes = 3, ignore_index = 0)\n",
        "            self.validation_metrics = torchmetrics.Dice(num_classes = 3, ignore_index = 0)\n",
        "            self.test_metrics = torchmetrics.Dice(num_classes = 3, ignore_index = 0)\n",
        "\n",
        "        self.fastMRI = fastMRI\n",
        "\n",
        "    def compute_metrics(self, metrics, pred, target):\n",
        "        if isinstance(metrics, nn.ModuleList):\n",
        "            for m in metrics[:-1]:\n",
        "                m.update(pred, target)\n",
        "            self.validation_nmse_errors.append(metrics[-1](pred, target))  # NMSE which is built from scratch is put at the end for simplicity\n",
        "        else:\n",
        "            metrics.update(pred, target)\n",
        "        return\n",
        "\n",
        "    def reset_metrics(self, metrics):\n",
        "        if isinstance(metrics, nn.ModuleList):\n",
        "            for m in metrics[:-1]:\n",
        "                m.reset()\n",
        "            self.validation_nmse_errors = []\n",
        "        else:\n",
        "            metrics.reset()\n",
        "\n",
        "    def forward_down_and_bottleneck(self, x):\n",
        "        x = self.patch_embeddings(x)\n",
        "        x_downsample = []\n",
        "\n",
        "        for layer in self.encoder:\n",
        "            x_downsample.append(x)\n",
        "            x = layer(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x, x_downsample\n",
        "\n",
        "    def forward_up(self, x, x_downsample):\n",
        "        x = self.decoder[0](x)\n",
        "        for idx, layer_up in enumerate(self.decoder[1:-1]):\n",
        "            i = idx+1\n",
        "            x = torch.cat([x, x_downsample[self.depths-i]],-1)\n",
        "            x = self.concat_linear[idx](x)\n",
        "            x = layer_up(x)\n",
        "\n",
        "        x = self.norm_up(x)\n",
        "        x = self.decoder[-1](x)\n",
        "\n",
        "        x = rearrange(x, \"b (h w) d -> b d h w\", h = 4*self.patch_resolution[0], w = 4*self.patch_resolution[1])\n",
        "        return self.output(x)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, x_downsample = self.forward_down_and_bottleneck(x)\n",
        "        return self.forward_up(x, x_downsample)\n",
        "\n",
        "    def inference(self, x):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            return self(x)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), self.learning_rate)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8, verbose=True)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch[:2]\n",
        "        y_pred = self(x)\n",
        "        if self.fastMRI:\n",
        "            y_pred = y_pred.clamp(0, 1)\n",
        "            bg_m = batch[-1]\n",
        "            y *= bg_m\n",
        "            y_pred *= bg_m\n",
        "\n",
        "        loss = self.loss_function(y_pred, y)\n",
        "        self.train_loss.append(loss)\n",
        "\n",
        "        if batch_idx % 100 == 0:  # layers set on eval to keep track of performance on training set\n",
        "            y_pred2 = self.inference(x)\n",
        "            self.train()\n",
        "            self.compute_metrics(self.train_metrics, y_pred2, y if self.fastMRI else y.type(torch.uint8))\n",
        "\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch[:2]\n",
        "        y_pred = self.inference(x)\n",
        "\n",
        "        if self.fastMRI:\n",
        "            y_pred = y_pred.clamp(0, 1)\n",
        "            bg_m = batch[-1]\n",
        "            y *= bg_m\n",
        "            y_pred *= bg_m\n",
        "        else:\n",
        "            y = y.type(torch.uint8)\n",
        "        self.compute_metrics(self.validation_metrics, y_pred, y)\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch[:2]\n",
        "        y_pred = self.inference(x)\n",
        "\n",
        "        if self.fastMRI:\n",
        "            y_pred = y_pred.clamp(0, 1)\n",
        "            bg_m = batch[-1]\n",
        "            y *= bg_m\n",
        "            y_pred *= bg_m\n",
        "        else:\n",
        "            y = y.type(torch.uint8)\n",
        "        self.compute_metrics(self.test_metrics, y_pred, y)\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        self.wandb_log['train_loss'] = sum(self.train_loss)/len(self.train_loss)\n",
        "        if self.fastMRI:\n",
        "            self.wandb_log['train_mae'] = self.train_metrics.compute()\n",
        "        else:\n",
        "            self.wandb_log['train_dicescore'] = self.train_metrics.compute()\n",
        "        self.wandb_log['epoch'] = self.epoch\n",
        "        self.epoch += 1\n",
        "        self.train_loss = []\n",
        "        self.reset_metrics(self.train_metrics)\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        if self.fastMRI:\n",
        "            self.log('val_mae_epoch', self.validation_metrics[0].compute(),\n",
        "                    on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "            self.wandb_log['validation_mae'] = self.validation_metrics[0].compute()\n",
        "            self.wandb_log['validation_psnr'] = self.validation_metrics[1].compute()\n",
        "            self.wandb_log['validation_ssim'] = self.validation_metrics[2].compute()\n",
        "            self.wandb_log['validation_nmse'] = sum(self.validation_nmse_errors)/len(self.validation_nmse_errors)\n",
        "        else:\n",
        "            self.log('val_dicescore_epoch', self.validation_metrics.compute(),\n",
        "                    on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "            self.wandb_log['validation_dicescore'] = self.validation_metrics.compute()\n",
        "\n",
        "        self.reset_metrics(self.validation_metrics)\n",
        "        wandb.log(self.wandb_log)\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        self.log('test_mae_epoch', self.test_mae.compute(),\n",
        "                 on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.wandb_log['test_mae'] = self.test_mae.compute()\n",
        "        self.test_mae.reset()\n"
      ],
      "metadata": {
        "id": "i4H5hEVyA7SO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# wandb setup"
      ],
      "metadata": {
        "id": "z6u_1-WgYMWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb --quiet"
      ],
      "metadata": {
        "id": "41TpWzcdrt1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "ODM9LRhaZ3S8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login API-KEY"
      ],
      "metadata": {
        "id": "PIr4gMFfqA_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training fastMRI"
      ],
      "metadata": {
        "id": "zcy8i-xGrvOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = \"./training/checkpoints\"\n",
        "checkpoint_dir = \"./training/checkpoints/c_swin_u_net\"\n",
        "#logger_dir = \"./training/tensorboard/c_swin_u_net\"\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 32\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_SIZE = 256\n",
        "# NUM_HEADS = 8\n",
        "# assert EMBEDDING_DIM % NUM_HEADS == 0\n",
        "\n",
        "DROP_PROB = 0.2\n",
        "GRADIENT_CLIP_VAL = 0.5\n",
        "use_iterative_algorithm = False\n",
        "eps = 2e-2\n",
        "eps_init = 4\n",
        "eps_decay = 0.7\n",
        "iters = 20\n",
        "depths=  [2, 2]\n",
        "num_heads = [4, 8, 16]\n",
        "# depths= [2]\n",
        "# num_heads = [4,8]"
      ],
      "metadata": {
        "id": "Z2dzDUOhacRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "now = datetime.now().strftime(\"%H.%M\")\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath = checkpoint_dir,\n",
        "    filename=now+'c_swin_u_net{epoch:02d}_{step:06d}_{val_mae_epoch:.3f}',\n",
        "    save_top_k=7,\n",
        "    monitor='val_mae_epoch',\n",
        "    mode='min',\n",
        "    verbose=True,\n",
        "    save_last=True\n",
        ")\n",
        "\n",
        "#logger = TensorBoardLogger(logger_dir, name=\"c_swin_u_net\")\n",
        "callbacks = [checkpoint_callback, TQDMProgressBar(refresh_rate=20)]\n",
        "trainer_input = {\n",
        "    \"default_root_dir\": root_dir,\n",
        "    \"accelerator\": \"auto\",\n",
        "    \"devices\": 1,\n",
        "    \"log_every_n_steps\": 50,\n",
        "    \"val_check_interval\": 1.0,\n",
        "    \"gradient_clip_val\": GRADIENT_CLIP_VAL,\n",
        "    \"max_epochs\": EPOCHS,\n",
        "    #\"logger\": logger,\n",
        "    \"callbacks\": callbacks,\n",
        "}\n",
        "\n",
        "datamodule_input = {\n",
        "    \"preprocessed_data_path\": \"./singlecoil_train_preprocessed\",\n",
        "    \"preprocessed_data_path_test\": \"./singlecoil_val_preprocessed\",\n",
        "    # \"original_data_path\": \"./singlecoil_train\",\n",
        "    # \"original_data_path_test\": \"./singlecoil_val\",\n",
        "    \"mask\": RectangularEquispacedMask(acceleration=4, central_ratio=0.8),\n",
        "    \"max_proc\": 1,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "}\n",
        "fastmri_dm = fastMRIDataModule(**datamodule_input)\n",
        "\n",
        "config_wandb = {\n",
        "    \"moby_pretrain\": \"none\",\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"embedding_dim\": EMBEDDING_DIM,\n",
        "    \"hidden_size\": HIDDEN_SIZE,\n",
        "    \"dropout\": 0.2,\n",
        "    \"depths\": depths,\n",
        "    \"num_heads\": num_heads,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"use_conditional_blocks\": True,\n",
        "    \"use_iterative_algorithm\": use_iterative_algorithm, \"eps\": eps, \"eps_init\": eps_init, \"eps_decay\": eps_decay, \"iters\": iters\n",
        "}"
      ],
      "metadata": {
        "id": "STXJAunitlk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project = \"fastMRI_training\", config = config_wandb)"
      ],
      "metadata": {
        "id": "otyuliQS1daB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(**trainer_input)\n",
        "swin_u_net = SwinTransformer_Unet(img_size = 256, patch_size = 4, input_channels = 1, embedding_dim = EMBEDDING_DIM,\n",
        "                                  hidden_size = HIDDEN_SIZE, window_size = 8, output_channels = 1, dropout = 0.2, norm_layer = nn.LayerNorm,\n",
        "                                  depths = depths, num_heads = num_heads, learning_rate = LEARNING_RATE, use_conditional_blocks=True,\n",
        "                                  use_iterative_algorithm = use_iterative_algorithm,\n",
        "                                  eps = eps, eps_init = eps_init, eps_decay = eps_decay, iters = iters)\n",
        "trainer.fit(swin_u_net, datamodule=fastmri_dm)"
      ],
      "metadata": {
        "id": "qyifeHo21XIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "W45ZbinNoDLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fastmri_dm.train_dataloader().__len__()"
      ],
      "metadata": {
        "id": "1T2c6oXXCOCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Liver Tumor Segmentation"
      ],
      "metadata": {
        "id": "vFnIPcav-xlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = \"./training/checkpoints\"\n",
        "checkpoint_dir = \"./training/checkpoints/swin_u_net\"\n",
        "#logger_dir = \"./training/tensorboard/c_swin_u_net\"\n",
        "EPOCHS = 30\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 32\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_SIZE = 256\n",
        "# NUM_HEADS = 8\n",
        "# assert EMBEDDING_DIM % NUM_HEADS == 0\n",
        "\n",
        "DROP_PROB = 0.2\n",
        "GRADIENT_CLIP_VAL = 0.5\n",
        "use_iterative_algorithm = False\n",
        "eps = 2e-2\n",
        "eps_init = 4\n",
        "eps_decay = 0.7\n",
        "iters = 20\n",
        "depths=  [2, 2]\n",
        "num_heads = [4, 8, 16]\n",
        "# depths= [2]\n",
        "# num_heads = [4,8]"
      ],
      "metadata": {
        "id": "LJ8un38W-xlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "now = datetime.now().strftime(\"%H.%M\")\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath = checkpoint_dir,\n",
        "    filename=now+'swin_u_net{epoch:02d}_{step:06d}_{val_dicescore_epoch:.3f}',\n",
        "    save_top_k=7,\n",
        "    monitor='val_dicescore_epoch',\n",
        "    mode='max',\n",
        "    verbose=True,\n",
        "    save_last=True\n",
        ")\n",
        "\n",
        "#logger = TensorBoardLogger(logger_dir, name=\"c_swin_u_net\")\n",
        "callbacks = [checkpoint_callback, TQDMProgressBar(refresh_rate=20)]\n",
        "trainer_input = {\n",
        "    \"default_root_dir\": root_dir,\n",
        "    \"accelerator\": \"auto\",\n",
        "    \"devices\": 1,\n",
        "    \"log_every_n_steps\": 50,\n",
        "    \"val_check_interval\": 1.0,\n",
        "    \"gradient_clip_val\": GRADIENT_CLIP_VAL,\n",
        "    \"max_epochs\": EPOCHS,\n",
        "    #\"logger\": logger,\n",
        "    \"callbacks\": callbacks,\n",
        "}\n",
        "\n",
        "datamodule_input = {\n",
        "    \"train_dataframe\": df_train,\n",
        "    \"validation_dataframe\": df_val,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "}\n",
        "livertumor_dm = LiverTumorSegmentationDataModule(**datamodule_input)\n",
        "\n",
        "config_wandb = {\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"embedding_dim\": EMBEDDING_DIM,\n",
        "    \"hidden_size\": HIDDEN_SIZE,\n",
        "    \"dropout\": 0.2,\n",
        "    \"depths\": depths,\n",
        "    \"num_heads\": num_heads,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"use_conditional_blocks\": False,\n",
        "    \"use_iterative_algorithm\": use_iterative_algorithm, \"eps\": eps, \"eps_init\": eps_init, \"eps_decay\": eps_decay, \"iters\": iters\n",
        "}"
      ],
      "metadata": {
        "id": "xCi6lAEt-xlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project = \"liver_dataset_training_new\", config = config_wandb)"
      ],
      "metadata": {
        "id": "nHn6dAPR-xlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(**trainer_input)\n",
        "swin_u_net = SwinTransformer_Unet(img_size = 256, patch_size = 4, input_channels = 3, embedding_dim = EMBEDDING_DIM,\n",
        "                                  hidden_size = HIDDEN_SIZE, window_size = 8, output_channels = 3, dropout = 0.2, norm_layer = nn.LayerNorm,\n",
        "                                  depths = depths, num_heads = num_heads, learning_rate = LEARNING_RATE, use_conditional_blocks=False,\n",
        "                                  use_iterative_algorithm = use_iterative_algorithm,\n",
        "                                  eps = eps, eps_init = eps_init, eps_decay = eps_decay, iters = iters, fastMRI = False)\n",
        "trainer.fit(swin_u_net, datamodule=livertumor_dm)"
      ],
      "metadata": {
        "id": "TBNi6DQm-xlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "9qc2V4py-xlF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}